---
title: "Fit a PLDA model to bill text with CRS tags"
author: 
  - ""
date: ""
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: show
    theme: paper
    highlight: tango
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 5)
proj_dir <- tryCatch(rprojroot::find_rstudio_root_file(),
                     error = function(e) rprojroot::find_root(".gitignore"))

knitr::opts_knit$set(root.dir = proj_dir)
```

```{r about, echo=FALSE, results = "asis"}
cat(
    sprintf("Updated: %s.", 
            format(Sys.time(), "%b %d, %Y at %H:%M:%S", usetz = TRUE)),
    sprintf("Working directory: `%s`.", getwd()),
    sep = "\n\n"
)
```

# Setup

```{r}
# renv::restore()
# reticulate::py_install(c("pyarrow", "tomotopy", "nltk", "pandas", "spacy", "spacy-transformers"))
```

```{python}
import json 
import copy 
import random
import numpy as np
import pyarrow
import pyarrow.parquet as pq
import pandas as pd
import tomotopy as tp

# Used for lemmatization
import spacy
# !python -m spacy download 'en_core_web_trf'
# !python -m spacy download 'en_core_web_sm'

# Used for stopwords
import nltk
# nltk.download("stopwords")

random.seed(575)
```


```{python}
crs_labels = json.load(open('data/topic_labels/crs_labels.json'))

bill_fn = 'data/legislation/govinfo/bill_text.parquet'
bills   = pyarrow.Table.to_pandas(pq.read_pandas(bill_fn))
```

Use a 50/50 train-test split, but move any unlabeled bills to the test set.

```{python}
n_train   = round(len(bills) / 2)
# index of 50/50 split
split_index = set(random.sample(range(1, len(bills)), n_train))
# index of unlabeled bills
unlabel_index = np.where([billid not in crs_labels.keys() for billid in bills['id']])[0].tolist()

# remove unlabeled bills from training set
train_index = list(set(split_index) - set(unlabel_index))
bills_train = bills.iloc[train_index]
bills_test = bills.drop(train_index)

len(bills_train); len(bills_test)

assert(len(bills_train) + len(bills_test) == len(bills))
```

How many bills moved from training split to testing due to missing label?
```{python}
len(split_index) - len(train_index)
```


```{python, corpus_params}
def spacy_lemma_tokens(raw, user_data, en = "en_core_web_sm"):
    # Load English model, keeping only tagger component needed for lemmatizing
    spacy_en = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])
    doc = spacy_en(raw)
    # Lemmatize and paste doc back together
    lemmas = [token.lemma_ for token in doc]
    out = " ".join(lemmas)
    return(out)


stopwords = set(nltk.corpus.stopwords.words('english'))

bill_corpus = tp.utils.Corpus(tokenizer = tp.utils.SimpleTokenizer(stemmer = spacy_lemma_tokens),
                              stopwords = lambda x: len(x) <= 2 or x in stopwords)

test_corpus = copy.copy(bill_corpus)
```


```{python, build_corpus_train}
bill_train_list = [(text, crs_labels[billid], {'labels':  crs_labels[billid]}) 
                   for text, billid in zip(bills_train['text'], bills_train['id'])
                   ]
 
bill_corpus.process(bill_train_list)
bill_corpus.save("data/legislation/corpus_train.pickle")
```


```{python, build_corpus_test}
bill_test_list = [(text, crs_labels[billid], {'labels':  crs_labels[billid]}) 
                   for text, billid in zip(bills_test['text'], bills_test['id'])
                   ]
 
test_corpus.process(bill_test_list)
test_corpus.save("data/legislation/corpus_train.pickle")
```


```{python}
mod_plda = tp.PLDAModel(tw = tp.TermWeight.IDF, corpus = bill_corpus, 
                        min_cf = 0, min_df = 100,
                        latent_topics = 1, topics_per_label = 1,
                        # hyperparameters for dirichlet
                        alpha = 0.1, eta = 0.01,
                        seed = 575)

mod_plda.train(iter = 100)
# mod_plda.train(iter = 5000)
```


```{python}
mod_plda.summary()
mod_plda.topic_label_dict
mod_plda.get_topic_words(topic_id=0)
```

